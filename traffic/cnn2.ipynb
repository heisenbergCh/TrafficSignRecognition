{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义是否使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 此标准集中类别名称和顺序，所谓的标签\n",
    "'''\n",
    "使得我们能够手动输入命令行参数，就是让风格变得和Linux命令行差不多\n",
    "argparse是python的一个包，用来解析输入的参数\n",
    "如：\n",
    "    python mnist.py --outf model  \n",
    "    （意思是将训练的模型保存到model文件夹下，当然，你也可以不加参数，那样的话代码最后一行\n",
    "      torch.save()就需要注释掉了）\n",
    "\n",
    "    python mnist.py --net model/net_005.pth\n",
    "    （意思是加载之前训练好的网络模型，前提是训练使用的网络和测试使用的网络是同一个网络模型，保证权重参数矩阵相等）\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--outf', default='./model/', help='folder to output images and model checkpoints')  # 模型保存路径\n",
    "parser.add_argument('--net', default='./model/net.pth', help=\"path to netG (to continue training)\")  # 模型加载路径\n",
    "opt,unknown = parser.parse_known_args()  # 解析得到你在路径中输入的参数，比如 --outf 后的\"model\"或者 --net 后的\"model/net_005.pth\"，是作为字符串形式保存的\n",
    "\n",
    "# Load training and testing datasets.\n",
    "ROOT_PATH = \"./data\"\n",
    "train_data_dir = os.path.join(ROOT_PATH, \"target_data/train\")\n",
    "test_data_dir = os.path.join(ROOT_PATH, \"target_data/test\")\n",
    "\n",
    "\n",
    "# 超参数设置\n",
    "BATCH_SIZE = 1     # 批处理尺寸(batch_size)：关于为何进行批处理，文档中有不错的介绍\n",
    "# cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "# cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "\n",
    "# 定义数据预处理方式(将输入的类似numpy中arrary形式的数据转化为pytorch中的张量（tensor）)\n",
    "# transform = transforms.ToTensor()\n",
    "# transform = torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), # 彩色图像转灰度图像num_output_channels默认1\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(120),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(0.5, 0.5, 0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 定义训练数据集\n",
    "trainset = tv.datasets.ImageFolder(train_data_dir, transform)\n",
    "\n",
    "# 定义训练批处理数据\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,                # 加载测试集\n",
    "    batch_size=BATCH_SIZE,   # 最小批处理尺寸\n",
    "    shuffle=True,            # 标识进行数据迭代时候将数据打乱\n",
    ")\n",
    "\n",
    "# 定义测试数据集\n",
    "testset = tv.datasets.ImageFolder(test_data_dir, transform)\n",
    "\n",
    "# 定义测试批处理数据\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,                 # 加载测试集\n",
    "    batch_size=BATCH_SIZE,   # 最小批处理尺寸\n",
    "    shuffle=True,           # 标识进行数据迭代时候将数据打乱\n",
    ")\n",
    "\n",
    "'''\n",
    "定义LeNet神经网络，进一步的理解可查看Pytorch入门，里面很详细，代码本质上是一样的，这里做了一些封装\n",
    "'''\n",
    "class LeNet(nn.Module):\n",
    "    '''\n",
    "    该类继承了torch.nn.Modul类\n",
    "    构建LeNet神经网络模型\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()  # 这一个是python中的调用父类LeNet的方法，因为LeNet继承了nn.Module，如果不加这一句，无法使用导入的torch.nn中的方法，这涉及到python的类继承问题，你暂时不用深究\n",
    "\n",
    "        # 第一层神经网络，包括卷积层、线性激活函数、池化层\n",
    "        self.conv1 = nn.Sequential(     # 输入层图片的输入尺寸(1*224*224)\n",
    "            nn.Conv2d(1, 6, 5),         # input_size=(1*224*224)\n",
    "            nn.ReLU(),                  # input_size=(6*220*220)：同上，其中的6是卷积后得到的通道个数，或者叫特征个数，进行ReLu激活\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # output_size=(6*110*110)：经过池化层后的输出\n",
    "        )\n",
    "\n",
    "        # 第二层神经网络，包括卷积层、线性激活函数、池化层\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, 5),  # input_size=(6*110*110)：  经过上一层池化层后的输出,作为第二层卷积层的输入，不采用填充方式进行卷积\n",
    "            nn.ReLU(),            # input_size=(16*106*106)： 对卷积神经网络的输出进行ReLu激活\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)    # output_size=(16*53*53)：  池化层后的输出结果\n",
    "        )\n",
    "\n",
    "        # 全连接层(将神经网络的神经元的多维输出转化为一维)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(16 * 53 * 53, 120),  # 进行线性变换\n",
    "            nn.ReLU()                    # 进行ReLu激活\n",
    "        )\n",
    "\n",
    "        # 输出层(将全连接层的一维输出进行处理)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 将输出层的数据进行分类(输出预测值)\n",
    "        self.fc3 = nn.Linear(84, 58)\n",
    "\n",
    "    # 定义前向传播过程，输入为x\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # nn.Linear()的输入输出都是维度为一的值，所以要把多维度的tensor展平成一维\n",
    "        x = x.view(-1, 16 * 53 * 53)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "EPOCH = 10   # 遍历数据集次数(训练模型的轮数)\n",
    "LR = 0.001        # 学习率：模型训练过程中每次优化的幅度\n",
    "\n",
    "def model_train():\n",
    "    # 定义损失函数loss function 和优化方式（采用SGD）\n",
    "    net = LeNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # 交叉熵损失函数，通常用于多分类问题上\n",
    "    optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)  # 优化函数\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        \n",
    "        sum_loss = 0.0\n",
    "        # 数据读取（采用python的枚举方法获得标签和数据，这一部分可能和numpy相关）\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            # labels = [torch.LongTensor(label) for label in labels]\n",
    "            # 将输入数据和标签放入构建的图中 注：图的概念可在pytorch入门中查\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward  注: 这一部分是训练神经网络的核心\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() # 反向自动求导\n",
    "            optimizer.step() # 进行优化\n",
    "\n",
    "            # 每训练100个batch打印一次平均loss\n",
    "            sum_loss += loss.item()\n",
    "            if i % 48 == 0:\n",
    "                print('[%d, %d] loss: %.03f'\n",
    "                      % (epoch + 1, i + 1, sum_loss / 100))\n",
    "                sum_loss = 0.0\n",
    "        # 每跑完一次epoch测试一下准确率\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # for i, data in enumerate(testloader):\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                # 取得分最高的那个类\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "            print('第%d个epoch的识别准确率为：%d%%' % (epoch + 1, (100 * correct / total)))\n",
    "    \n",
    "    torch.save(net, '%s/net_%03d.pth' % (opt.outf, epoch + 1))\n",
    "    \n",
    "# 训练\n",
    "model_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # 画图库\n",
    "\n",
    "def imshow(img):\n",
    "    # 增强图片对比度，使暗的地方更亮\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    # 图片转换为nparrary \n",
    "    npimg = img.numpy()\n",
    "    # 图片显示，此处需要转置，因为pytorch与numpy多维顺序结构问题\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# 从训练集中取出一些数据\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "# imshow(tv.utils.make_grid(images))\n",
    "\n",
    "classes = range(58)\n",
    "\n",
    "#载入模型\n",
    "model_save_path = 'model/net_040.pth'\n",
    "model = torch.load(model_save_path)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# 得到预测结果，并且从大到小排序\n",
    "imshow(tv.utils.make_grid(images))\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "_, predicted = torch.max(out, 1)\n",
    "# imshow(tv.utils.make_grid(images))\n",
    "print('lable:    ',' '.join('%5s' % classes[labels[j]] for j in range(3)))\n",
    "print('Predicted:', ' '.join('%5s' % classes[predicted[j]] for j in range(3)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66ff79579bafc981d3ebb3637f85874de6bb2fbcfcda94e92f9e8660a35f8d72"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('baoxing-pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
